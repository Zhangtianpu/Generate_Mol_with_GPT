import torch
import argparse
import numpy as np
from model.LLM_model import llm_model
import Utils
from SmilesTokenizer import SmilesTokenzier
import yaml


def init_model(args, vocab):
    BASE_CONFIG = {
        "vocab_size": len(vocab),
        "context_length": args.context_length,
        "drop_rate": 0.0,
        "bias": True
    }
    model_config = {
        "gpt2_124M": {'emb_dim': 768, "n_layers": 12, "n_heads": 12, "head_dim": 64},
        "gpt2_335M": {'emb_dim': 1024, 'n_layers': 24, "n_heads": 16, "head_dim": 64},
        "gpt2_774M": {"emb_dim": 1280, 'n_layers': 36, "n_heads": 20, "head_dim": 64},
        "gpt2_1.5B": {"emb_dim": 1600, 'n_layers': 48, 'n_heads': 25, 'head_dim': 64}
    }
    CHOOSE_MODEL = args.choose_model
    if CHOOSE_MODEL == 'custom':
        yaml_path = '../model_params/custom_params.yml'
        with open(yaml_path, 'r') as f:
            yaml_config = yaml.safe_load(f)
        custom_model_parameter = yaml_config['model_parameter']
        BASE_CONFIG.update(custom_model_parameter)
    else:
        BASE_CONFIG.update(model_config[CHOOSE_MODEL])

    llm = llm_model(vocab_len=BASE_CONFIG['vocab_size'],
                    context_length=BASE_CONFIG['context_length'],
                    wordEmbedding_dim=BASE_CONFIG['emb_dim'],
                    positionEmbedding_dim=BASE_CONFIG['emb_dim'],
                    n_layers=BASE_CONFIG['n_layers'],
                    head_dim=BASE_CONFIG['head_dim'], num_head=BASE_CONFIG['n_heads'],
                    device=args.rank, drop_rate=BASE_CONFIG['drop_rate'])
    return llm


if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    parser = argparse.ArgumentParser(description="generate mol with llm")
    """
    parameters for vocab
    """
    parser.add_argument('--vocab_folder', type=str, default='../vocab')
    parser.add_argument('--vocab_name', type=str, default='vocab.json')

    """
    training parameters
    """
    parser.add_argument('--rank', type=str, default='cpu')
    parser.add_argument('--seed', type=int, default=123)
    """
    model's parameters
    """
    parser.add_argument('--save_model_path', type=str,
                        default='../trained_model/mol_llm.pkl')
    parser.add_argument('--choose_model', type=str, default='custom')
    """
    generation parameters
    """
    parser.add_argument('--context_length', type=int, default=147)
    parser.add_argument('--max_new_tokens', type=int, default=147)
    parser.add_argument('--temperature', type=float, default=1.5)
    parser.add_argument('--top_K', type=int, default=15, help="Integer or None")
    parser.add_argument('--eos_id', type=int, default=90)
    parser.add_argument('--numbers', type=int, default=100,help="The number of smile generated by model")
    args = parser.parse_args()
    args.rank = device
    # torch.cuda.manual_seed_all(args.seed)
    # np.random.seed(args.seed)

    """
    load vocab and llm model
    """
    vocab = Utils.load_vocab(args.vocab_folder, args.vocab_name)
    llm = init_model(args, vocab)
    Utils.load_model(llm, args.save_model_path)
    llm.to(device)
    start_context = "CC"
    tokenizer = SmilesTokenzier(vocab)

    start_context_ids = Utils.text_to_ids(start_context, tokenizer).to(device)
    generated_ids_list = Utils.generate_for_numbers(args.numbers, llm, start_context_ids,
                                                    args.max_new_tokens,
                                                    args.context_length,
                                                    args.temperature,
                                                    args.top_K, args.eos_id)
    # model, idx, max_new_tokens, context_size, temperature = 0.0, top_K = None, eos_id = None
    # generated_ids = Utils.generate(llm, start_context_ids,
    #                                args.max_new_tokens,
    #                                args.context_length,
    #                                args.temperature,
    #                                args.top_K, args.eos_id)
    generated_smiles = Utils.idslist_to_text(generated_ids_list, tokenizer)
    Utils.show_from_smiles(generated_smiles)
